{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression model with CV and lasso regularisation \n",
    "\n",
    "**Questions :**\n",
    "- What number of iterations to ensure convergence ?\n",
    "- What lasso regularisation parameter ?\n",
    "- What cross-validation method ?\n",
    "- Validation/in/out sample sizes influence\n",
    "\n",
    "\n",
    "**ISSUES :**\n",
    "- missing values in the eeg : drop or try to replace ?\n",
    "- convergence of the model is very slow\n",
    "- model performance (whether accuracy, MSE or Kullback-Liebler divergence)\n",
    "\n",
    "**Notes :**\n",
    "- no significant class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.preamble import *\n",
    "\n",
    "train_eeg_names = os.listdir(Dir.eeg_train)\n",
    "train_spc_names = os.listdir(Dir.spc_train)\n",
    "len(train_eeg_names), len(train_spc_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.classes import Eeg, ChainBuilder, EegChain, FeatureGenerator\n",
    "from source.scoring import score\n",
    "from source.train_algos import (\n",
    "    train_logistic_regression_CV,\n",
    "    test_model,\n",
    "    predict_probas_test_set,\n",
    ")\n",
    "from source.process import (\n",
    "    open_train_metadata,\n",
    "    print_summary_metadata,\n",
    "    convert_parquet_to_npy,\n",
    ")\n",
    "from source.pre_train import extract_validation_set\n",
    "from source.plotting import plot_coefs, plot_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_all = open_train_metadata(read=False)\n",
    "print_summary_metadata(meta_all)\n",
    "meta_all.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert parquet to npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_npy_conversion = False\n",
    "if run_npy_conversion:\n",
    "    convert_parquet_to_npy(Dir.eeg_train, Dir.eeg_train, meta_all[\"eeg_id\"].unique())\n",
    "    run_npy_conversion = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample class usage\n",
    "For Eeg class :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg = Eeg(Dir.eeg_train, meta_all.iloc[1])\n",
    "print(\"EEG sub id : \", eeg.eeg_sub_id)\n",
    "display(eeg.open())  # the whole EEG\n",
    "display(eeg.open_subs())  # only the selected subsample\n",
    "eeg.plot(columns=[\"Fp1\", \"T6\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For FeatureGenerator and EegChain : \n",
    "- cascading methods\n",
    "- reusable object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeg_chain_train(sample):\n",
    "    return (\n",
    "        EegChain()\n",
    "        .open(Eeg(Dir.eeg_train, sample))\n",
    "        ._fillna()\n",
    "        ._divide(coef=10000.0)\n",
    "        .mean(cols=[\"Fp1\", \"EKG\"])\n",
    "        .var(cols=[\"EKG\"])\n",
    "        .signature(\n",
    "            cols=[\"Fp1\", \"EKG\", \"F7\", \"T3\", \"O2\"],\n",
    "            depth=3,\n",
    "            index=[i for i in range(ChainBuilder.n_sig_coordinates(5, 3))],\n",
    "            time_augment=True,\n",
    "        )\n",
    "        .result()\n",
    "    )\n",
    "\n",
    "\n",
    "feature_generator = FeatureGenerator(eeg_chain=eeg_chain_train)\n",
    "\n",
    "meta_sample = meta_all.iloc[:100]\n",
    "\n",
    "features = feature_generator.process(metadata=meta_sample, save=False)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "- make sure not to generate class imbalance\n",
    "- seed should be fixed in local for debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split validation\n",
    "rest_meta, validation_meta = extract_validation_set(meta_all, ratio=0.05)\n",
    "print_summary_metadata(rest_meta)\n",
    "print_summary_metadata(validation_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "max_nsample = 100 if KAGGLE else 10000\n",
    "max_it = 20000\n",
    "cs = 0.08  # 0.08 for only a few parameters\n",
    "\n",
    "sig_cols = [\"Fp1\", \"EKG\"]\n",
    "pre_norm_coef = 10e4\n",
    "depth = 3\n",
    "\n",
    "\n",
    "def eeg_chain_train(sample):\n",
    "    return (\n",
    "        EegChain()\n",
    "        .open_npy(Eeg(Dir.eeg_train, sample))\n",
    "        # ._center()\n",
    "        # ._fillna()\n",
    "        .mean_npy(cols=eeg_name_to_idx(EEG_COLS))\n",
    "        .var_npy(cols=eeg_name_to_idx(EEG_COLS))\n",
    "        #     ._divide(coef=pre_norm_coef)\n",
    "        #     .signature_npy(\n",
    "        #         cols=eeg_name_to_idx(sig_cols),\n",
    "        #         depth=depth,\n",
    "        #         time_augment=True,\n",
    "        #         factorial_rescale=True\n",
    "        # )\n",
    "        .result()\n",
    "    )\n",
    "\n",
    "\n",
    "feature_generator = FeatureGenerator(\n",
    "    eeg_chain=eeg_chain_train,\n",
    "    # save=os.path.join(Dir.intermediate_output, \"eeg_features_train.parquet\"),\n",
    ")\n",
    "\n",
    "model, *other = train_logistic_regression_CV(\n",
    "    rest_meta,\n",
    "    feature_generator,\n",
    "    VOTE_COLS,\n",
    "    max_it=max_it,\n",
    "    max_nsample=max_nsample,\n",
    "    grade=Grade.bad,\n",
    "    scale=True,\n",
    "    Cs=[cs],\n",
    "    fit_intercept=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the same preprocessing for the test and validation data\n",
    "scaler = other[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coefs(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune lasso regularisation parameter\n",
    "\n",
    "- with Logistic regression **10 fold CV** and **Z-score scaling** \n",
    "- 1000 train samples\n",
    "- max solver iteration = 1000 (saga)\n",
    "- **CONVERGENCE ISSUES** (max iteration reached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KAGGLE:\n",
    "    target_probas = validation_meta[VOTE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and optimisation params\n",
    "max_nsample = 1000 if KAGGLE else 1000\n",
    "max_it = 1000\n",
    "\n",
    "# penalisation\n",
    "csx = np.linspace(0.1, 1, 2, endpoint=True)\n",
    "\n",
    "\n",
    "# feature spacce\n",
    "def eeg_chain_train(sample):\n",
    "    return (\n",
    "        EegChain()\n",
    "        .open_npy(Eeg(Dir.eeg_train, sample))\n",
    "        # ._center()\n",
    "        # ._fillna()\n",
    "        .mean_npy(cols=eeg_name_to_idx(EEG_COLS))\n",
    "        .var_npy(cols=eeg_name_to_idx(EEG_COLS))\n",
    "        #     ._divide(coef=pre_norm_coef)\n",
    "        #     .signature_npy(\n",
    "        #         cols=eeg_name_to_idx(sig_cols),\n",
    "        #         depth=depth,\n",
    "        #         time_augment=True,\n",
    "        #         factorial_rescale=True\n",
    "        # )\n",
    "        .result()\n",
    "    )\n",
    "\n",
    "\n",
    "feature_generator = FeatureGenerator(\n",
    "    eeg_chain=eeg_chain_train,\n",
    ")\n",
    "\n",
    "\n",
    "for cs in csx:\n",
    "    print(\"=\" * 100)\n",
    "    print(\"\\tCs=\", cs)\n",
    "    model, *other = train_logistic_regression_CV(\n",
    "        rest_meta,\n",
    "        feature_generator,\n",
    "        VOTE_COLS,\n",
    "        max_it=max_it,\n",
    "        max_nsample=max_nsample,\n",
    "        grade=Grade.certain,\n",
    "        scale=True,\n",
    "        Cs=[cs],\n",
    "    )\n",
    "    scaler = other[0]\n",
    "    predicted_probas = test_model(model, feature_generator, VOTE_COLS, validation_meta, scaler)\n",
    "    fig, _ = plot_coefs(model)\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Not yet defined how to deal with NA values in the signals \n",
    "=> they are skipped for now\n",
    "\n",
    "Also need to fix the computation of the subsample length (at each change of file)\n",
    "=> those are skipped as well\n",
    "\n",
    "\"clean\" => keep only data that has no issue\n",
    "In the future should not drop any test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we want to get rid of outliars ?\n",
    "validation_meta_clean = validation_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KAGGLE:\n",
    "    feature_generator = FeatureGenerator(\n",
    "        eeg_chain=eeg_chain_train,\n",
    "        # save=os.path.join(Dir.intermediate_output, \"eeg_features_test.parquet\"),\n",
    "    )\n",
    "    predicted_probas = test_model(\n",
    "        model, feature_generator, VOTE_COLS, validation_meta_clean, scaler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIFORM PREDICTED PROBAS\n",
    "# most basic benchmark\n",
    "pp = [1.0 / 6] * 6\n",
    "predicted_probas_uniform = np.array([pp for _ in range(len(validation_meta_clean))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : \n",
    "the uniform benchmark is not even outperformed.\n",
    "\n",
    "Most likely because the data is very noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not KAGGLE:\n",
    "    target_probas = validation_meta_clean[VOTE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_proba_predict = (\n",
    "    predicted_probas == np.repeat(np.max(predicted_probas, axis=1), 6).reshape((-1, 6))\n",
    ").astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(predicted_probas_uniform, target_probas.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(predicted_probas, target_probas.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> confusion matrix to visualise which classes are better classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(max_proba_predict, target_probas.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(predicted_probas, target_probas.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(max_proba_predict, target_probas.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The linear regression sucks : it didn't budge from uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Regularisation is usefull in our case as many features are redundant and many of them don't have predictive power.\n",
    "- It it still very inefficient : it is very similar to the uniform predictor\n",
    "\n",
    "This indicates that even in the time-augmented feature-space, the classes are not linearly independant.\n",
    "\n",
    "It could be interesting not to perform the time-augmentation so that the scaling of patterns would be matched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the true test samples\n",
    "meta_test = pd.read_csv(os.path.join(Dir.root, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_parquet_to_npy(Dir.eeg_test, Dir.eeg_test, meta_test[\"eeg_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the objects for the submission folder\n",
    "def eeg_chain_test(sample):\n",
    "    return (\n",
    "        EegChain()\n",
    "        .open_npy(Eeg(Dir.eeg_test, sample), subsample=False)\n",
    "        # ._center()\n",
    "        # ._fillna()\n",
    "        .mean_npy(cols=eeg_name_to_idx(EEG_COLS))\n",
    "        .var_npy(cols=eeg_name_to_idx(EEG_COLS))\n",
    "        #     ._divide(coef=pre_norm_coef)\n",
    "        #     .signature_npy(\n",
    "        #         cols=eeg_name_to_idx(sig_cols),\n",
    "        #         depth=depth,\n",
    "        #         time_augment=True,\n",
    "        #         factorial_rescale=True\n",
    "        # )\n",
    "        .result()\n",
    "    )\n",
    "\n",
    "\n",
    "feature_generator_test = FeatureGenerator(\n",
    "    eeg_chain=eeg_chain_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = predict_probas_test_set(model, meta_test, feature_generator_test)\n",
    "sub.to_csv(os.path.join(Dir.out, \"submission.csv\"))\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.iloc[0] = (sub.values == np.repeat(np.max(sub.values), len(VOTE_COLS))).astype(float)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
